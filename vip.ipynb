{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3e2225",
   "metadata": {},
   "source": [
    "## Replace entities with placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9062af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/aiotlab/envs/linh_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model và tokenizer\n",
    "model_name = \"linhdzqua148/xlm-roberta-ner-japanese-railway\"\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(model_name)\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "id_to_label = {0: 'O', 1: 'B-STATION', 2: 'I-STATION', 3: 'B-LINE', 4: 'I-LINE'}\n",
    "\n",
    "def predict_entities(text_tokens, model, tokenizer):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text_tokens,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    word_ids = inputs.word_ids()\n",
    "    predicted_labels = []\n",
    "    seen_words = set()\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is not None and word_id not in seen_words:\n",
    "            predicted_labels.append(id_to_label[predictions[0][i].item()])\n",
    "            seen_words.add(word_id)\n",
    "    min_len = min(len(text_tokens), len(predicted_labels))\n",
    "    return list(zip(text_tokens[:min_len], predicted_labels[:min_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ba4184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MeCab hoạt động\n",
      "['これ', 'は', 'テスト', 'です']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "try:\n",
    "    mecab = MeCab.Tagger(\"-Owakati\") \n",
    "    print(\"✅ MeCab hoạt động\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Lỗi:\", e)\n",
    "\n",
    "def improved_tokenize(text):\n",
    "    tokens = mecab.parse(text).strip().split()\n",
    "    return tokens\n",
    "\n",
    "print(improved_tokenize(\"これはテストです\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e302fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_entity(entity_text):\n",
    "    \"\"\"\n",
    "    Normalize entity: strip suffix + split number\n",
    "    \"\"\"\n",
    "    SUFFIXES_TO_STRIP = [\"方面行き\", \"方面\", \"行き\"]\n",
    "    NUM_SPLIT_PAT = re.compile(r\"^(.+?)(\\d+号)$\")\n",
    "    \n",
    "    for suffix in SUFFIXES_TO_STRIP:\n",
    "        if entity_text.endswith(suffix):\n",
    "            entity_text = entity_text[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    match = NUM_SPLIT_PAT.match(entity_text)\n",
    "    if match:\n",
    "        base, suffix = match.groups()\n",
    "        return [base, suffix]\n",
    "    else:\n",
    "        return [entity_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86774a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_normalize_entities(predicted):\n",
    "    \"\"\"\n",
    "    Trích xuất entities và normalize chúng ngay từ đầu\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(predicted):\n",
    "        token, label = predicted[i]\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            ent_type = label.split(\"-\", 1)[1]\n",
    "            ent_tokens = [token]\n",
    "            i += 1\n",
    "            \n",
    "            while i < len(predicted) and predicted[i][1] == f\"I-{ent_type}\":\n",
    "                ent_tokens.append(predicted[i][0])\n",
    "                i += 1\n",
    "            \n",
    "            entity_text = \"\".join(ent_tokens)\n",
    "            \n",
    "            # Normalize entity ngay tại đây\n",
    "            normalized_entities = normalize_entity(entity_text)\n",
    "            entities.extend(normalized_entities)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholder_mapping(text, entities):\n",
    "    \"\"\"\n",
    "    Tạo mapping giữa entity và placeholder dựa trên vị trí xuất hiện\n",
    "    \"\"\"\n",
    "    offsets = []\n",
    "    for ent in entities:\n",
    "        pos = text.find(ent)\n",
    "        if pos != -1:\n",
    "            offsets.append((pos, ent))\n",
    "    \n",
    "    offsets.sort() \n",
    "    \n",
    "    ph2ent = {}\n",
    "    ent2ph = {}\n",
    "    ph_counter = 0\n",
    "    \n",
    "    for pos, ent in offsets:\n",
    "        if ent not in ent2ph:\n",
    "            ph = f\"[PH{ph_counter}]\"\n",
    "            ent2ph[ent] = ph\n",
    "            ph2ent[ph] = ent\n",
    "            ph_counter += 1\n",
    "    \n",
    "    return ph2ent, ent2ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849c72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_text_with_placeholders(text, ent2ph):\n",
    "    \"\"\"\n",
    "    Thay thế entities trong text bằng placeholders\n",
    "    \"\"\"\n",
    "    result_text = text\n",
    "    \n",
    "    sorted_entities = sorted(ent2ph.keys(), key=len, reverse=True)\n",
    "    \n",
    "    for ent in sorted_entities:\n",
    "        result_text = result_text.replace(ent, ent2ph[ent])\n",
    "    \n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7936d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity_and_map(txt: str):\n",
    "   \n",
    "    try:\n",
    "        # Tokenize\n",
    "        tokens = improved_tokenize(txt)\n",
    "        \n",
    "        # Predict entities\n",
    "        predicted = predict_entities(tokens, model_ner, tokenizer_ner)\n",
    "        \n",
    "        # Extract và normalize entities\n",
    "        normalized_entities = extract_and_normalize_entities(predicted)\n",
    "        \n",
    "        # Tạo mapping placeholder\n",
    "        ph2ent, ent2ph = create_placeholder_mapping(txt, normalized_entities)\n",
    "        \n",
    "        # Mask text\n",
    "        final_text = mask_text_with_placeholders(txt, ent2ph)\n",
    "        \n",
    "        return final_text, ph2ent\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi xử lý: {e}\")\n",
    "        return txt, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233d0ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ま', 'O'), ('も', 'O'), ('なく', 'O'), ('、', 'B-STATION'), ('品川', 'B-STATION'), ('、', 'B-STATION'), ('品川', 'B-STATION'), ('。', 'O'), ('お', 'O'), ('出口', 'O'), ('は', 'O'), ('右側', 'O'), ('です', 'O'), ('。', 'B-LINE'), ('京浜', 'B-LINE'), ('東北', 'I-LINE'), ('線', 'I-LINE'), ('、', 'B-LINE'), ('山手', 'B-LINE'), ('線', 'I-LINE'), ('、', 'B-LINE'), ('京急', 'B-LINE'), ('線', 'I-LINE'), ('は', 'O'), ('お', 'O'), ('乗り換え', 'O'), ('です', 'O'), ('。', 'O'), ('本日', 'O'), ('も', 'B-LINE'), ('JR', 'B-LINE'), ('東', 'I-LINE'), ('日本', 'I-LINE'), ('を', 'O'), ('ご', 'O'), ('利用', 'O'), ('ください', 'O'), ('まし', 'O'), ('て', 'O'), ('、', 'O'), ('ありがとう', 'O'), ('ござい', 'I-LINE'), ('まし', 'I-LINE'), ('た', 'I-LINE'), ('。', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ま[PH0]なく[PH1][PH2][PH1][PH2][PH3]お出口は右側です[PH3][PH4][PH1][PH5][PH1][PH6]はお乗り換えです[PH3]本日[PH0][PH7]をご利用くださいまして[PH1]ありがとうございました[PH3]',\n",
       " {'[PH0]': 'も',\n",
       "  '[PH1]': '、',\n",
       "  '[PH2]': '品川',\n",
       "  '[PH3]': '。',\n",
       "  '[PH4]': '京浜東北線',\n",
       "  '[PH5]': '山手線',\n",
       "  '[PH6]': '京急線',\n",
       "  '[PH7]': 'JR東日本'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = input()\n",
    "print(predict_entities( improved_tokenize(txt), model_ner, tokenizer_ner  ))\n",
    "\n",
    "replace_entity_and_map(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55c877",
   "metadata": {},
   "source": [
    "## dịch máy và dịch thực thể"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193037ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/aiotlab/envs/linh_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import jaconv\n",
    "from fugashi import Tagger\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/aiotlab/envs/linh_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'linhdzqua148/opus-mt-ja-en-railway-7' loaded successfully on cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "model_name = \"linhdzqua148/jrw-mt-ja-en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Check if CUDA is available\n",
    "try:\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer = tok\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    print(f\"Model '{model_name}' loaded successfully on {device}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    device = \"cpu\"\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer = tok\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "        print(f\"Model loaded on CPU due to error on GPU.\")\n",
    "    except Exception as cpu_e:\n",
    "        print(f\"Error loading model on CPU as well: {cpu_e}\")\n",
    "        tokenizer = None\n",
    "        model = None\n",
    "        tagger = None # Disable tagger if model loading fails critically\n",
    "        print(\"Translation functionality will be limited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f630152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_entity_mapping_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Đọc file CSV và tạo dictionary mapping giống như function database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"📁 Đang đọc file CSV...\")\n",
    "        # Đọc file CSV trên Kaggle\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        \n",
    "        print(f\"📊 Đã đọc {len(df)} dòng dữ liệu\")\n",
    "        \n",
    "        # Loại bỏ dữ liệu null/empty giống như logic database\n",
    "        df_clean = df.dropna(subset=['kanji', 'english'])\n",
    "        df_clean = df_clean[(df_clean['kanji'] != '') & (df_clean['english'] != '')]\n",
    "        \n",
    "        # Tạo dictionary mapping: kanji -> english\n",
    "        entity_direct_map_csv = dict(zip(df_clean['kanji'], df_clean['english']))\n",
    "        \n",
    "        print(f\"✅ Hoàn tất! Tổng số entity: {len(entity_direct_map_csv)}\")\n",
    "        \n",
    "        return entity_direct_map_csv\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ae33ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Đang đọc file CSV...\n",
      "📊 Đã đọc 19174 dòng dữ liệu\n",
      "✅ Hoàn tất! Tổng số entity: 18596\n",
      "Dictionary size: 18596\n"
     ]
    }
   ],
   "source": [
    "entity_direct_map_csv = get_entity_mapping_from_csv('./train_entity.csv')\n",
    "print(f\"Dictionary size: {len(entity_direct_map_csv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b06039f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_name_from_wikidata(japanese_name):\n",
    "    \"\"\"\n",
    "    Tìm tên tiếng Anh từ Wikidata cho entities đường sắt\n",
    "    Chỉ xử lý các entities hợp lệ cho ngữ cảnh đường sắt\n",
    "    \"\"\"\n",
    "    # Filter 1: Loại bỏ các trường hợp không hợp lệ\n",
    "    if not japanese_name or len(japanese_name.strip()) < 2:\n",
    "        return None\n",
    "    \n",
    "    japanese_name = japanese_name.strip()\n",
    "    \n",
    "    # Filter 2: Loại bỏ dấu câu, ký tự đơn lẻ và các từ không phải entity\n",
    "    punctuation_chars = {'。', '、', '，', '．', '！', '？', '：', '；'}\n",
    "    if japanese_name in punctuation_chars:\n",
    "        return None\n",
    "    \n",
    "    # Filter 3: Loại bỏ các hạt từ (particles) và từ chức năng tiếng Nhật\n",
    "    japanese_particles = {\n",
    "        'は', 'を', 'が', 'に', 'で', 'と', 'の', 'か', 'から', 'まで', 'も', 'へ', 'より', 'だけ', 'ばかり',\n",
    "        'くらい', 'ほど', 'など', 'しか', 'でも', 'だって', 'って', 'なら', 'たら', 'れば', 'けれど',\n",
    "        'が', 'けど', 'のに', 'ので', 'から', 'ため', 'ように', 'ために', 'として', 'について',\n",
    "        'によって', 'に関して', 'に対して', 'について', 'として', 'において', 'において'\n",
    "    }\n",
    "    if japanese_name in japanese_particles:\n",
    "        return None\n",
    "    \n",
    "    # Filter 4: Loại bỏ các từ đơn lẻ không có ý nghĩa entity\n",
    "    single_meaningless_chars = {\n",
    "        'あ', 'い', 'う', 'え', 'お', 'か', 'き', 'く', 'け', 'こ', 'さ', 'し', 'す', 'せ', 'そ',\n",
    "        'た', 'ち', 'つ', 'て', 'と', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ひ', 'ふ', 'へ', 'ほ',\n",
    "        'ま', 'み', 'む', 'め', 'も', 'や', 'ゆ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん'\n",
    "    }\n",
    "    if japanese_name in single_meaningless_chars:\n",
    "        return None\n",
    "    \n",
    "    # Filter 5: Chỉ xử lý nếu có ký tự chỉ định đường sắt\n",
    "    railway_indicators = ['駅', '線', 'メトロ', 'Metro', '鉄道', '電車', '新幹線', 'JR', '方面', '地下鉄']\n",
    "    has_railway_indicator = any(indicator in japanese_name for indicator in railway_indicators)\n",
    "    \n",
    "    # Filter 6: Kiểm tra độ dài hợp lý cho tên ga/tuyến\n",
    "    is_reasonable_length = len(japanese_name) >= 2 and len(japanese_name) <= 25\n",
    "    \n",
    "    # Chỉ tiếp tục nếu có chỉ định đường sắt VÀ độ dài hợp lý\n",
    "    if not (has_railway_indicator and is_reasonable_length):\n",
    "        return None\n",
    "    \n",
    "    search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"language\": \"ja\",\n",
    "        \"format\": \"json\",\n",
    "        \"search\": japanese_name,\n",
    "        \"limit\": 5 # Get a few results to be safer\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Added explicit headers to mimic a browser request, might help with some firewalls/blocks\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(search_url, params=search_params, timeout=8, headers=headers) # Increased timeout slightly\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        results = response.json().get(\"search\", [])\n",
    "\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        # Pick the first result và kiểm tra description\n",
    "        entity_id = results[0][\"id\"]\n",
    "        description = results[0].get(\"description\", \"\").lower()\n",
    "        \n",
    "        # Filter 4: Kiểm tra description có liên quan đến đường sắt không\n",
    "        railway_keywords = ['station', 'railway', 'train', 'metro', 'line', 'subway', 'transit']\n",
    "        is_railway_related = any(keyword in description for keyword in railway_keywords) if description else True\n",
    "        \n",
    "        # Nếu có description nhưng không liên quan đến đường sắt, bỏ qua\n",
    "        if description and not is_railway_related and not has_railway_indicator:\n",
    "            return None\n",
    "\n",
    "        data_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
    "\n",
    "        response = requests.get(data_url, timeout=8, headers=headers) # Added timeout and headers\n",
    "        response.raise_for_status()\n",
    "\n",
    "        entity_data = response.json()\n",
    "\n",
    "        # Try to get the English label\n",
    "        en_label = entity_data.get(\"entities\", {}).get(entity_id, {}).get(\"labels\", {}).get(\"en\", {}).get(\"value\")\n",
    "\n",
    "        if en_label:\n",
    "             return en_label\n",
    "        else:\n",
    "             return None # Found entity but no English label\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Wikidata request timed out for '{japanese_name}'.\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Wikidata connection error for '{japanese_name}'. Network issue?\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "         print(f\"Wikidata HTTP error for '{japanese_name}': {e}\")\n",
    "         return None\n",
    "    except ValueError: # JSON decode error\n",
    "        print(f\"Wikidata response is not valid JSON for '{japanese_name}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Wikidata lookup for '{japanese_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dffb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_entity(jp_entity):\n",
    "    \"\"\"\n",
    "    Translates a Japanese entity name (like station, line, etc.) to English\n",
    "    using a prioritized approach:\n",
    "    1. Custom CSV Dictionary lookup (Kanji -> English) - Highest priority for specific names\n",
    "    2. Wikidata lookup - Good for known proper nouns not in CSV\n",
    "    \"\"\"\n",
    "    jp_entity = jp_entity.strip()\n",
    "    if not jp_entity:\n",
    "        return \"\" \n",
    "    \n",
    "    print(f\"Translating entity: '{jp_entity}'\") # Debug print\n",
    "\n",
    "    # 1) Tra từ điển tùy chỉnh (CSV)\n",
    "    if jp_entity in entity_direct_map_csv:\n",
    "        print(f\" -> Matched in CSV dictionary: {entity_direct_map_csv[jp_entity]}\")\n",
    "        return entity_direct_map_csv[jp_entity]\n",
    "\n",
    "    # 2) Tra từ Wikidata\n",
    "    wikidata_result = get_en_name_from_wikidata(jp_entity)\n",
    "    if wikidata_result:\n",
    "        print(f\" -> Matched in Wikidata: {wikidata_result}\")\n",
    "        return wikidata_result\n",
    "\n",
    "    # 3) Fallback to machine translation\n",
    "    return jp_entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75ab4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\"Chia câu theo '。', giữ dấu cuối để ghép lại mạch lạc.\"\"\"\n",
    "    # This function seems designed for splitting input text for NMT batching,\n",
    "    # not directly for entity translation.\n",
    "    parts = [p.strip() for p in text.split(\"。\") if p.strip()]\n",
    "    return [p + \"。\" for p in parts]\n",
    "\n",
    "def translate_text(text_list, batch_size=16):\n",
    "    \"\"\"Dịch list câu Nhật → Anh bằng model.generate() (Batch Processing)\"\"\"\n",
    "    # This function is for batch processing sentences, not used by process_row currently.\n",
    "    if not isinstance(text_list, list):\n",
    "        print(\"Error: translate_text expects a list of strings.\")\n",
    "        return [] # Or raise an error\n",
    "    if model is None or tokenizer is None:\n",
    "         print(\"Error: Model or tokenizer not loaded. Cannot perform batch translation.\")\n",
    "         return [\"Error: Translator not available.\"] * len(text_list)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    # Use the global 'tokenizer' variable\n",
    "    # Note: Splitting by '。' might not be ideal for short railway announcements.\n",
    "    # Consider if you need this splitting logic here or just pass full sentences.\n",
    "    # Current code does splitting.\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i : i + batch_size]\n",
    "        seg_pairs = []\n",
    "        concat_map = [] # maps segment index back to original batch_texts index\n",
    "        for idx, txt in enumerate(batch_texts):\n",
    "            # Assuming split_sentences is appropriate here\n",
    "            segs = split_sentences(txt)\n",
    "            seg_pairs.extend(segs)\n",
    "            concat_map.extend([idx] * len(segs))\n",
    "\n",
    "        # Handle empty seg_pairs if batch_texts were empty or didn't split\n",
    "        if not seg_pairs:\n",
    "            results.extend([\"\"] * len(batch_texts))\n",
    "            continue\n",
    "\n",
    "        # Encode all segments\n",
    "        try:\n",
    "            inputs = tokenizer(seg_pairs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=128,\n",
    "                    num_beams=6,\n",
    "                    length_penalty=0.8,\n",
    "                    # Add other generation args if needed, matching process_row direct call\n",
    "                )\n",
    "            decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch NMT translation for batch starting with '{batch_texts[0]}': {e}\")\n",
    "\n",
    "        # Merge segments back into original sentence structure\n",
    "        merged = [\"\"] * len(batch_texts)\n",
    "        for seg, idx in zip(decoded, concat_map):\n",
    "             # Careful merging: add space only if the merged string is not empty\n",
    "             if merged[idx]:\n",
    "                 merged[idx] += (\" \" + seg.strip())\n",
    "             else:\n",
    "                 merged[idx] += seg.strip()\n",
    "\n",
    "        # Final cleanup for merged sentences\n",
    "        merged = [re.sub(r'\\s+([.,!?:;])', r'\\1', s).strip() for s in merged] # Punctuation spacing\n",
    "        merged = [re.sub(r'\\s+', ' ', s).strip() for s in merged] # Normalize spaces\n",
    "        merged = [s.replace(' .', '.') for s in merged] # Fix common space before period\n",
    "\n",
    "\n",
    "        results.extend(merged)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5b08213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_adjacent_duplicate_phrases(text, max_phrase_len=5):\n",
    "    \"\"\"\n",
    "    Loại bỏ các cụm từ (từ 1 đến max_phrase_len từ) lặp liên tiếp nhau trong văn bản.\n",
    "    Hỗ trợ lặp có hoặc không có dấu phẩy, không phân biệt hoa thường.\n",
    "    \"\"\"\n",
    "    # Loại bỏ khoảng trắng thừa ở dấu phẩy\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # Xử lý cụm từ lặp, giảm dần từ cụm dài nhất về 1 từ\n",
    "    for n in range(max_phrase_len, 0, -1):\n",
    "        # Tạo regex động cho cụm n từ lặp liên tiếp (có thể có dấu phẩy)\n",
    "        # Ví dụ cho n=2: (Tokyo Metro)(,? Tokyo Metro)\n",
    "        pattern = re.compile(\n",
    "            r'(\\b(?:[\\w\\-\\’ōū]+(?:\\s+|, ?)){%d}[\\w\\-\\’ōū]+\\b)'    # Nhóm 1: n từ\n",
    "            r'(,? \\1\\b)'                                          # Nhóm 2: lặp lại\n",
    "            % (n-1),\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "        # Lặp để loại tất cả các trường hợp lặp liên tiếp\n",
    "        while pattern.search(text):\n",
    "            text = pattern.sub(r'\\1', text)\n",
    "    # Sau đó, xử lý riêng lặp 1 từ với dấu phẩy hoặc không dấu phẩy\n",
    "    text = re.sub(r'\\b(\\w+)(,? \\1\\b)', r'\\1', text, flags=re.IGNORECASE)\n",
    "    # Sửa các trường hợp dấu câu dư, khoảng trắng thừa\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bdb8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_simple(text):\n",
    "    \"\"\"\n",
    "    Dịch văn bản đơn giản bằng machine translation model\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=6,\n",
    "                length_penalty=0.8,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        result = tokenizer.decode(generated[0], skip_special_tokens=True).strip()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Translation error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a037d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_entities_with_fallback(ph2ent):\n",
    "    \"\"\"\n",
    "    Dịch entities với logic:\n",
    "    1. Check trong CSV trước\n",
    "    2. Không có thì check bằng hàm Wiki\n",
    "    3. Không có thì thay ngược lại từ đó thay cho placeholder đã thay trong câu và xóa nó trong placeholder map\n",
    "    \n",
    "    Args:\n",
    "        ph2ent: Dictionary mapping placeholder -> entity (ví dụ: {'[PH0]': 'は', '[PH1]': '大阪'})\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (updated_ph2ent, entities_to_restore)\n",
    "        - updated_ph2ent: Dictionary đã loại bỏ entities không dịch được\n",
    "        - entities_to_restore: Dictionary mapping placeholder -> entity để thay ngược lại\n",
    "    \"\"\"\n",
    "    updated_ph2ent = {}\n",
    "    entities_to_restore = {}\n",
    "    \n",
    "    print(\"🔄 Bắt đầu dịch entities...\")\n",
    "    \n",
    "    for placeholder, entity in ph2ent.items():\n",
    "        print(f\"\\n📝 Xử lý {placeholder} -> '{entity}'\")\n",
    "        \n",
    "        # Bước 1: Check trong CSV\n",
    "        if entity in entity_direct_map_csv:\n",
    "            translated = entity_direct_map_csv[entity]\n",
    "            updated_ph2ent[placeholder] = translated\n",
    "            print(f\"  ✅ Tìm thấy trong CSV: '{entity}' -> '{translated}'\")\n",
    "            continue\n",
    "        \n",
    "        # Bước 2: Check bằng hàm Wiki\n",
    "        wikidata_result = get_en_name_from_wikidata(entity)\n",
    "        if wikidata_result:\n",
    "            updated_ph2ent[placeholder] = wikidata_result\n",
    "            print(f\"  ✅ Tìm thấy trong Wiki: '{entity}' -> '{wikidata_result}'\")\n",
    "            continue\n",
    "        \n",
    "        # Bước 3: Không tìm được -> thay ngược lại\n",
    "        entities_to_restore[placeholder] = entity\n",
    "        print(f\"  ❌ Không tìm được bản dịch cho '{entity}' -> sẽ thay ngược lại\")\n",
    "    \n",
    "    print(f\"\\n📊 Kết quả:\")\n",
    "    print(f\"  - Entities dịch được: {len(updated_ph2ent)}\")\n",
    "    print(f\"  - Entities thay ngược lại: {len(entities_to_restore)}\")\n",
    "    \n",
    "    return updated_ph2ent, entities_to_restore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80157098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_entities_and_translate(text_with_placeholders, entities_to_restore, translated_entities):\n",
    "    \"\"\"\n",
    "    Thay ngược lại các entities không dịch được và dịch câu\n",
    "    \n",
    "    Args:\n",
    "        text_with_placeholders: Câu đã thay placeholder (ví dụ: \"この電車[PH0][PH1]行き快速電車です。\")\n",
    "        entities_to_restore: Dictionary mapping placeholder -> entity để thay ngược lại\n",
    "        translated_entities: Dictionary mapping placeholder -> entity đã dịch\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (final_text, final_ph2ent)\n",
    "        - final_text: Câu đã thay ngược lại entities không dịch được\n",
    "        - final_ph2ent: Dictionary chỉ chứa entities đã dịch được\n",
    "    \"\"\"\n",
    "    print(\"🔄 Thay ngược lại entities không dịch được...\")\n",
    "    \n",
    "    # Bước 1: Thay ngược lại entities không dịch được\n",
    "    final_text = text_with_placeholders\n",
    "    for placeholder, original_entity in entities_to_restore.items():\n",
    "        final_text = final_text.replace(placeholder, original_entity)\n",
    "        print(f\"  🔄 Thay ngược {placeholder} -> '{original_entity}'\")\n",
    "    \n",
    "    print(f\"📝 Text sau khi thay ngược: {final_text}\")\n",
    "    \n",
    "    # Bước 2: Dịch câu\n",
    "    print(\"🔄 Dịch câu...\")\n",
    "    translated_text = translate_text_simple(final_text)\n",
    "    print(f\"📝 Câu đã dịch: {translated_text}\")\n",
    "    \n",
    "    # Bước 3: Tạo final mapping chỉ chứa entities đã dịch được\n",
    "    final_ph2ent = translated_entities.copy()\n",
    "    \n",
    "    return final_text, translated_text, final_ph2ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a69fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_translation_with_entities(translated_text, final_ph2ent):\n",
    "    \"\"\"\n",
    "    Ghép lại kết quả dịch với entities đã dịch dựa trên placeholder mapping\n",
    "    \n",
    "    Args:\n",
    "        translated_text: Câu đã dịch (ví dụ: \"This train is a rapid train bound for [PH1]. Next is [PH2], [PH2].\")\n",
    "        final_ph2ent: Dictionary mapping placeholder -> entity đã dịch (ví dụ: {'[PH1]': 'Osaka', '[PH2]': 'Kyoto'})\n",
    "    \n",
    "    Returns:\n",
    "        str: Câu cuối cùng đã ghép entities\n",
    "    \"\"\"\n",
    "    print(\"🔄 Ghép lại kết quả với entities đã dịch...\")\n",
    "    \n",
    "    final_result = translated_text\n",
    "    \n",
    "    # Thay thế các placeholder bằng entities đã dịch\n",
    "    for placeholder, translated_entity in final_ph2ent.items():\n",
    "        if placeholder in final_result:\n",
    "            final_result = final_result.replace(placeholder, translated_entity)\n",
    "            print(f\"  ✅ Thay {placeholder} -> '{translated_entity}'\")\n",
    "    \n",
    "    # Loại bỏ các placeholder còn lại (nếu có)\n",
    "    import re\n",
    "    remaining_placeholders = re.findall(r'\\[PH\\d+\\]', final_result)\n",
    "    if remaining_placeholders:\n",
    "        print(f\"  ⚠️ Còn lại placeholders: {remaining_placeholders}\")\n",
    "    \n",
    "    print(f\"📝 Kết quả cuối cùng: {final_result}\")\n",
    "    \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ad893a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_entity_handling(text):\n",
    "    \"\"\"\n",
    "    Hàm tổng hợp thực hiện toàn bộ flow dịch với xử lý entities:\n",
    "    \n",
    "    1. Thay thế entities bằng placeholders\n",
    "    2. Dịch entities (check CSV -> Wiki -> thay ngược lại)\n",
    "    3. Dịch câu đã thay placeholder\n",
    "    4. Ghép lại kết quả với entities đã dịch\n",
    "    \n",
    "    Args:\n",
    "        text: Câu tiếng Nhật cần dịch\n",
    "    \n",
    "    Returns:\n",
    "        str: Câu tiếng Anh đã dịch hoàn chỉnh\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 BẮT ĐẦU QUÁ TRÌNH DỊCH VỚI XỬ LÝ ENTITIES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📝 Input: {text}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Bước 1: Thay thế entities bằng placeholders\n",
    "        print(\"🔸 BƯỚC 1: Thay thế entities bằng placeholders\")\n",
    "        text_with_placeholders, ph2ent = replace_entity_and_map(text)\n",
    "        print(f\"📝 Text sau khi thay placeholder: {text_with_placeholders}\")\n",
    "        print(f\"🗺️ Placeholder mapping: {ph2ent}\")\n",
    "        print()\n",
    "        \n",
    "        if not ph2ent:\n",
    "            print(\"⚠️ Không tìm thấy entities, dịch trực tiếp...\")\n",
    "            return translate_text_simple(text)\n",
    "        \n",
    "        # Bước 2: Dịch entities\n",
    "        print(\"🔸 BƯỚC 2: Dịch entities\")\n",
    "        translated_entities, entities_to_restore = translate_entities_with_fallback(ph2ent)\n",
    "        print()\n",
    "        \n",
    "        # Bước 3: Thay ngược lại và dịch câu\n",
    "        print(\"🔸 BƯỚC 3: Thay ngược lại entities không dịch được và dịch câu\")\n",
    "        final_text, translated_text, final_ph2ent = restore_entities_and_translate(\n",
    "            text_with_placeholders, entities_to_restore, translated_entities\n",
    "        )\n",
    "        print()\n",
    "        \n",
    "        # Bước 4: Ghép lại kết quả\n",
    "        print(\"🔸 BƯỚC 4: Ghép lại kết quả với entities đã dịch\")\n",
    "        final_result = merge_translation_with_entities(translated_text, final_ph2ent)\n",
    "        print()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"✅ HOÀN THÀNH QUÁ TRÌNH DỊCH\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📝 Kết quả cuối cùng: {final_result}\")\n",
    "        \n",
    "        return final_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi trong quá trình dịch: {e}\")\n",
    "        print(\"🔄 Fallback: Dịch trực tiếp...\")\n",
    "        return translate_text_simple(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b15ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TEST VỚI VÍ DỤ TỪ YÊU CẦU\n",
      "==================================================\n",
      "============================================================\n",
      "🚀 BẮT ĐẦU QUÁ TRÌNH DỊCH VỚI XỬ LÝ ENTITIES\n",
      "============================================================\n",
      "📝 Input:  まもなく、品川、品川。お出口は右側です。京浜東北線、山手線、京急線はお乗り換えです。本日もJR東日本をご利用くださいまして、ありがとうございました。\n",
      "\n",
      "🔸 BƯỚC 1: Thay thế entities bằng placeholders\n",
      "📝 Text sau khi thay placeholder:  ま[PH0]なく[PH1][PH2][PH1][PH2][PH3]お出口は右側です[PH3][PH4][PH1][PH5][PH1][PH6]はお乗り換えです[PH3]本日[PH0][PH7]をご利用くださいまして[PH1]ありがとうございました[PH3]\n",
      "🗺️ Placeholder mapping: {'[PH0]': 'も', '[PH1]': '、', '[PH2]': '品川', '[PH3]': '。', '[PH4]': '京浜東北線', '[PH5]': '山手線', '[PH6]': '京急線', '[PH7]': 'JR東日本'}\n",
      "\n",
      "🔸 BƯỚC 2: Dịch entities\n",
      "🔄 Bắt đầu dịch entities...\n",
      "\n",
      "📝 Xử lý [PH0] -> 'も'\n",
      "  ❌ Không tìm được bản dịch cho 'も' -> sẽ thay ngược lại\n",
      "\n",
      "📝 Xử lý [PH1] -> '、'\n",
      "  ❌ Không tìm được bản dịch cho '、' -> sẽ thay ngược lại\n",
      "\n",
      "📝 Xử lý [PH2] -> '品川'\n",
      "  ✅ Tìm thấy trong CSV: '品川' -> 'Shinagawa'\n",
      "\n",
      "📝 Xử lý [PH3] -> '。'\n",
      "  ❌ Không tìm được bản dịch cho '。' -> sẽ thay ngược lại\n",
      "\n",
      "📝 Xử lý [PH4] -> '京浜東北線'\n",
      "  ✅ Tìm thấy trong CSV: '京浜東北線' -> 'Keihin–Tōhoku Line'\n",
      "\n",
      "📝 Xử lý [PH5] -> '山手線'\n",
      "  ✅ Tìm thấy trong CSV: '山手線' -> 'Yamate Line'\n",
      "\n",
      "📝 Xử lý [PH6] -> '京急線'\n",
      "  ✅ Tìm thấy trong Wiki: '京急線' -> 'Keikyū Main Line'\n",
      "\n",
      "📝 Xử lý [PH7] -> 'JR東日本'\n",
      "  ✅ Tìm thấy trong Wiki: '京急線' -> 'Keikyū Main Line'\n",
      "\n",
      "📝 Xử lý [PH7] -> 'JR東日本'\n",
      "  ✅ Tìm thấy trong Wiki: 'JR東日本' -> 'East Japan Railway Company'\n",
      "\n",
      "📊 Kết quả:\n",
      "  - Entities dịch được: 5\n",
      "  - Entities thay ngược lại: 3\n",
      "\n",
      "🔸 BƯỚC 3: Thay ngược lại entities không dịch được và dịch câu\n",
      "🔄 Thay ngược lại entities không dịch được...\n",
      "  🔄 Thay ngược [PH0] -> 'も'\n",
      "  🔄 Thay ngược [PH1] -> '、'\n",
      "  🔄 Thay ngược [PH3] -> '。'\n",
      "📝 Text sau khi thay ngược:  まもなく、[PH2]、[PH2]。お出口は右側です。[PH4]、[PH5]、[PH6]はお乗り換えです。本日も[PH7]をご利用くださいまして、ありがとうございました。\n",
      "🔄 Dịch câu...\n",
      "  ✅ Tìm thấy trong Wiki: 'JR東日本' -> 'East Japan Railway Company'\n",
      "\n",
      "📊 Kết quả:\n",
      "  - Entities dịch được: 5\n",
      "  - Entities thay ngược lại: 3\n",
      "\n",
      "🔸 BƯỚC 3: Thay ngược lại entities không dịch được và dịch câu\n",
      "🔄 Thay ngược lại entities không dịch được...\n",
      "  🔄 Thay ngược [PH0] -> 'も'\n",
      "  🔄 Thay ngược [PH1] -> '、'\n",
      "  🔄 Thay ngược [PH3] -> '。'\n",
      "📝 Text sau khi thay ngược:  まもなく、[PH2]、[PH2]。お出口は右側です。[PH4]、[PH5]、[PH6]はお乗り換えです。本日も[PH7]をご利用くださいまして、ありがとうございました。\n",
      "🔄 Dịch câu...\n",
      "📝 Câu đã dịch: We will soon be arriving at [PH2], [PH2]. The doors will open on the right. Please transfer here for the [PH4], [PH5], and [PH6]. Thank you for traveling with [PH7] today.\n",
      "\n",
      "🔸 BƯỚC 4: Ghép lại kết quả với entities đã dịch\n",
      "🔄 Ghép lại kết quả với entities đã dịch...\n",
      "  ✅ Thay [PH2] -> 'Shinagawa'\n",
      "  ✅ Thay [PH4] -> 'Keihin–Tōhoku Line'\n",
      "  ✅ Thay [PH5] -> 'Yamate Line'\n",
      "  ✅ Thay [PH6] -> 'Keikyū Main Line'\n",
      "  ✅ Thay [PH7] -> 'East Japan Railway Company'\n",
      "📝 Kết quả cuối cùng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihin–Tōhoku Line, Yamate Line, and Keikyū Main Line. Thank you for traveling with East Japan Railway Company today.\n",
      "\n",
      "============================================================\n",
      "✅ HOÀN THÀNH QUÁ TRÌNH DỊCH\n",
      "============================================================\n",
      "📝 Kết quả cuối cùng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihin–Tōhoku Line, Yamate Line, and Keikyū Main Line. Thank you for traveling with East Japan Railway Company today.\n",
      "📝 Câu đã dịch: We will soon be arriving at [PH2], [PH2]. The doors will open on the right. Please transfer here for the [PH4], [PH5], and [PH6]. Thank you for traveling with [PH7] today.\n",
      "\n",
      "🔸 BƯỚC 4: Ghép lại kết quả với entities đã dịch\n",
      "🔄 Ghép lại kết quả với entities đã dịch...\n",
      "  ✅ Thay [PH2] -> 'Shinagawa'\n",
      "  ✅ Thay [PH4] -> 'Keihin–Tōhoku Line'\n",
      "  ✅ Thay [PH5] -> 'Yamate Line'\n",
      "  ✅ Thay [PH6] -> 'Keikyū Main Line'\n",
      "  ✅ Thay [PH7] -> 'East Japan Railway Company'\n",
      "📝 Kết quả cuối cùng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihin–Tōhoku Line, Yamate Line, and Keikyū Main Line. Thank you for traveling with East Japan Railway Company today.\n",
      "\n",
      "============================================================\n",
      "✅ HOÀN THÀNH QUÁ TRÌNH DỊCH\n",
      "============================================================\n",
      "📝 Kết quả cuối cùng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihin–Tōhoku Line, Yamate Line, and Keikyū Main Line. Thank you for traveling with East Japan Railway Company today.\n"
     ]
    }
   ],
   "source": [
    "# Test với ví dụ từ yêu cầu\n",
    "test_text = \" まもなく、品川、品川。お出口は右側です。京浜東北線、山手線、京急線はお乗り換えです。本日もJR東日本をご利用くださいまして、ありがとうございました。\"\n",
    "\n",
    "print(\"🧪 TEST VỚI VÍ DỤ TỪ YÊU CẦU\")\n",
    "print(\"=\" * 50)\n",
    "result = translate_with_entity_handling(test_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
