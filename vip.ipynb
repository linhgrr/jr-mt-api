{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3e2225",
   "metadata": {},
   "source": [
    "## Replace entities with placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9062af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/aiotlab/envs/linh_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model vÃ  tokenizer\n",
    "model_name = \"linhdzqua148/xlm-roberta-ner-japanese-railway\"\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(model_name)\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "id_to_label = {0: 'O', 1: 'B-STATION', 2: 'I-STATION', 3: 'B-LINE', 4: 'I-LINE'}\n",
    "\n",
    "def predict_entities(text_tokens, model, tokenizer):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text_tokens,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    word_ids = inputs.word_ids()\n",
    "    predicted_labels = []\n",
    "    seen_words = set()\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is not None and word_id not in seen_words:\n",
    "            predicted_labels.append(id_to_label[predictions[0][i].item()])\n",
    "            seen_words.add(word_id)\n",
    "    min_len = min(len(text_tokens), len(predicted_labels))\n",
    "    return list(zip(text_tokens[:min_len], predicted_labels[:min_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ba4184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MeCab hoáº¡t Ä‘á»™ng\n",
      "['ã“ã‚Œ', 'ã¯', 'ãƒ†ã‚¹ãƒˆ', 'ã§ã™']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "try:\n",
    "    mecab = MeCab.Tagger(\"-Owakati\") \n",
    "    print(\"âœ… MeCab hoáº¡t Ä‘á»™ng\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Lá»—i:\", e)\n",
    "\n",
    "def improved_tokenize(text):\n",
    "    tokens = mecab.parse(text).strip().split()\n",
    "    return tokens\n",
    "\n",
    "print(improved_tokenize(\"ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e302fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_entity(entity_text):\n",
    "    \"\"\"\n",
    "    Normalize entity: strip suffix + split number\n",
    "    \"\"\"\n",
    "    SUFFIXES_TO_STRIP = [\"æ–¹é¢è¡Œã\", \"æ–¹é¢\", \"è¡Œã\"]\n",
    "    NUM_SPLIT_PAT = re.compile(r\"^(.+?)(\\d+å·)$\")\n",
    "    \n",
    "    for suffix in SUFFIXES_TO_STRIP:\n",
    "        if entity_text.endswith(suffix):\n",
    "            entity_text = entity_text[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    match = NUM_SPLIT_PAT.match(entity_text)\n",
    "    if match:\n",
    "        base, suffix = match.groups()\n",
    "        return [base, suffix]\n",
    "    else:\n",
    "        return [entity_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86774a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_normalize_entities(predicted):\n",
    "    \"\"\"\n",
    "    TrÃ­ch xuáº¥t entities vÃ  normalize chÃºng ngay tá»« Ä‘áº§u\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(predicted):\n",
    "        token, label = predicted[i]\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            ent_type = label.split(\"-\", 1)[1]\n",
    "            ent_tokens = [token]\n",
    "            i += 1\n",
    "            \n",
    "            while i < len(predicted) and predicted[i][1] == f\"I-{ent_type}\":\n",
    "                ent_tokens.append(predicted[i][0])\n",
    "                i += 1\n",
    "            \n",
    "            entity_text = \"\".join(ent_tokens)\n",
    "            \n",
    "            # Normalize entity ngay táº¡i Ä‘Ã¢y\n",
    "            normalized_entities = normalize_entity(entity_text)\n",
    "            entities.extend(normalized_entities)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholder_mapping(text, entities):\n",
    "    \"\"\"\n",
    "    Táº¡o mapping giá»¯a entity vÃ  placeholder dá»±a trÃªn vá»‹ trÃ­ xuáº¥t hiá»‡n\n",
    "    \"\"\"\n",
    "    offsets = []\n",
    "    for ent in entities:\n",
    "        pos = text.find(ent)\n",
    "        if pos != -1:\n",
    "            offsets.append((pos, ent))\n",
    "    \n",
    "    offsets.sort() \n",
    "    \n",
    "    ph2ent = {}\n",
    "    ent2ph = {}\n",
    "    ph_counter = 0\n",
    "    \n",
    "    for pos, ent in offsets:\n",
    "        if ent not in ent2ph:\n",
    "            ph = f\"[PH{ph_counter}]\"\n",
    "            ent2ph[ent] = ph\n",
    "            ph2ent[ph] = ent\n",
    "            ph_counter += 1\n",
    "    \n",
    "    return ph2ent, ent2ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849c72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_text_with_placeholders(text, ent2ph):\n",
    "    \"\"\"\n",
    "    Thay tháº¿ entities trong text báº±ng placeholders\n",
    "    \"\"\"\n",
    "    result_text = text\n",
    "    \n",
    "    sorted_entities = sorted(ent2ph.keys(), key=len, reverse=True)\n",
    "    \n",
    "    for ent in sorted_entities:\n",
    "        result_text = result_text.replace(ent, ent2ph[ent])\n",
    "    \n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7936d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity_and_map(txt: str):\n",
    "   \n",
    "    try:\n",
    "        # Tokenize\n",
    "        tokens = improved_tokenize(txt)\n",
    "        \n",
    "        # Predict entities\n",
    "        predicted = predict_entities(tokens, model_ner, tokenizer_ner)\n",
    "        \n",
    "        # Extract vÃ  normalize entities\n",
    "        normalized_entities = extract_and_normalize_entities(predicted)\n",
    "        \n",
    "        # Táº¡o mapping placeholder\n",
    "        ph2ent, ent2ph = create_placeholder_mapping(txt, normalized_entities)\n",
    "        \n",
    "        # Mask text\n",
    "        final_text = mask_text_with_placeholders(txt, ent2ph)\n",
    "        \n",
    "        return final_text, ph2ent\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i xá»­ lÃ½: {e}\")\n",
    "        return txt, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233d0ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ã¾', 'O'), ('ã‚‚', 'O'), ('ãªã', 'O'), ('ã€', 'B-STATION'), ('å“å·', 'B-STATION'), ('ã€', 'B-STATION'), ('å“å·', 'B-STATION'), ('ã€‚', 'O'), ('ãŠ', 'O'), ('å‡ºå£', 'O'), ('ã¯', 'O'), ('å³å´', 'O'), ('ã§ã™', 'O'), ('ã€‚', 'B-LINE'), ('äº¬æµœ', 'B-LINE'), ('æ±åŒ—', 'I-LINE'), ('ç·š', 'I-LINE'), ('ã€', 'B-LINE'), ('å±±æ‰‹', 'B-LINE'), ('ç·š', 'I-LINE'), ('ã€', 'B-LINE'), ('äº¬æ€¥', 'B-LINE'), ('ç·š', 'I-LINE'), ('ã¯', 'O'), ('ãŠ', 'O'), ('ä¹—ã‚Šæ›ãˆ', 'O'), ('ã§ã™', 'O'), ('ã€‚', 'O'), ('æœ¬æ—¥', 'O'), ('ã‚‚', 'B-LINE'), ('JR', 'B-LINE'), ('æ±', 'I-LINE'), ('æ—¥æœ¬', 'I-LINE'), ('ã‚’', 'O'), ('ã”', 'O'), ('åˆ©ç”¨', 'O'), ('ãã ã•ã„', 'O'), ('ã¾ã—', 'O'), ('ã¦', 'O'), ('ã€', 'O'), ('ã‚ã‚ŠãŒã¨ã†', 'O'), ('ã”ã–ã„', 'I-LINE'), ('ã¾ã—', 'I-LINE'), ('ãŸ', 'I-LINE'), ('ã€‚', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ã¾[PH0]ãªã[PH1][PH2][PH1][PH2][PH3]ãŠå‡ºå£ã¯å³å´ã§ã™[PH3][PH4][PH1][PH5][PH1][PH6]ã¯ãŠä¹—ã‚Šæ›ãˆã§ã™[PH3]æœ¬æ—¥[PH0][PH7]ã‚’ã”åˆ©ç”¨ãã ã•ã„ã¾ã—ã¦[PH1]ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ[PH3]',\n",
       " {'[PH0]': 'ã‚‚',\n",
       "  '[PH1]': 'ã€',\n",
       "  '[PH2]': 'å“å·',\n",
       "  '[PH3]': 'ã€‚',\n",
       "  '[PH4]': 'äº¬æµœæ±åŒ—ç·š',\n",
       "  '[PH5]': 'å±±æ‰‹ç·š',\n",
       "  '[PH6]': 'äº¬æ€¥ç·š',\n",
       "  '[PH7]': 'JRæ±æ—¥æœ¬'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = input()\n",
    "print(predict_entities( improved_tokenize(txt), model_ner, tokenizer_ner  ))\n",
    "\n",
    "replace_entity_and_map(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55c877",
   "metadata": {},
   "source": [
    "## dá»‹ch mÃ¡y vÃ  dá»‹ch thá»±c thá»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193037ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/aiotlab/envs/linh_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import jaconv\n",
    "from fugashi import Tagger\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/aiotlab/envs/linh_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'linhdzqua148/opus-mt-ja-en-railway-7' loaded successfully on cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "model_name = \"linhdzqua148/jrw-mt-ja-en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Check if CUDA is available\n",
    "try:\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer = tok\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    print(f\"Model '{model_name}' loaded successfully on {device}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    device = \"cpu\"\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer = tok\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "        print(f\"Model loaded on CPU due to error on GPU.\")\n",
    "    except Exception as cpu_e:\n",
    "        print(f\"Error loading model on CPU as well: {cpu_e}\")\n",
    "        tokenizer = None\n",
    "        model = None\n",
    "        tagger = None # Disable tagger if model loading fails critically\n",
    "        print(\"Translation functionality will be limited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f630152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_entity_mapping_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Äá»c file CSV vÃ  táº¡o dictionary mapping giá»‘ng nhÆ° function database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ“ Äang Ä‘á»c file CSV...\")\n",
    "        # Äá»c file CSV trÃªn Kaggle\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        \n",
    "        print(f\"ğŸ“Š ÄÃ£ Ä‘á»c {len(df)} dÃ²ng dá»¯ liá»‡u\")\n",
    "        \n",
    "        # Loáº¡i bá» dá»¯ liá»‡u null/empty giá»‘ng nhÆ° logic database\n",
    "        df_clean = df.dropna(subset=['kanji', 'english'])\n",
    "        df_clean = df_clean[(df_clean['kanji'] != '') & (df_clean['english'] != '')]\n",
    "        \n",
    "        # Táº¡o dictionary mapping: kanji -> english\n",
    "        entity_direct_map_csv = dict(zip(df_clean['kanji'], df_clean['english']))\n",
    "        \n",
    "        print(f\"âœ… HoÃ n táº¥t! Tá»•ng sá»‘ entity: {len(entity_direct_map_csv)}\")\n",
    "        \n",
    "        return entity_direct_map_csv\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ae33ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Äang Ä‘á»c file CSV...\n",
      "ğŸ“Š ÄÃ£ Ä‘á»c 19174 dÃ²ng dá»¯ liá»‡u\n",
      "âœ… HoÃ n táº¥t! Tá»•ng sá»‘ entity: 18596\n",
      "Dictionary size: 18596\n"
     ]
    }
   ],
   "source": [
    "entity_direct_map_csv = get_entity_mapping_from_csv('./train_entity.csv')\n",
    "print(f\"Dictionary size: {len(entity_direct_map_csv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b06039f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_name_from_wikidata(japanese_name):\n",
    "    \"\"\"\n",
    "    TÃ¬m tÃªn tiáº¿ng Anh tá»« Wikidata cho entities Ä‘Æ°á»ng sáº¯t\n",
    "    Chá»‰ xá»­ lÃ½ cÃ¡c entities há»£p lá»‡ cho ngá»¯ cáº£nh Ä‘Æ°á»ng sáº¯t\n",
    "    \"\"\"\n",
    "    # Filter 1: Loáº¡i bá» cÃ¡c trÆ°á»ng há»£p khÃ´ng há»£p lá»‡\n",
    "    if not japanese_name or len(japanese_name.strip()) < 2:\n",
    "        return None\n",
    "    \n",
    "    japanese_name = japanese_name.strip()\n",
    "    \n",
    "    # Filter 2: Loáº¡i bá» dáº¥u cÃ¢u, kÃ½ tá»± Ä‘Æ¡n láº» vÃ  cÃ¡c tá»« khÃ´ng pháº£i entity\n",
    "    punctuation_chars = {'ã€‚', 'ã€', 'ï¼Œ', 'ï¼', 'ï¼', 'ï¼Ÿ', 'ï¼š', 'ï¼›'}\n",
    "    if japanese_name in punctuation_chars:\n",
    "        return None\n",
    "    \n",
    "    # Filter 3: Loáº¡i bá» cÃ¡c háº¡t tá»« (particles) vÃ  tá»« chá»©c nÄƒng tiáº¿ng Nháº­t\n",
    "    japanese_particles = {\n",
    "        'ã¯', 'ã‚’', 'ãŒ', 'ã«', 'ã§', 'ã¨', 'ã®', 'ã‹', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚‚', 'ã¸', 'ã‚ˆã‚Š', 'ã ã‘', 'ã°ã‹ã‚Š',\n",
    "        'ãã‚‰ã„', 'ã»ã©', 'ãªã©', 'ã—ã‹', 'ã§ã‚‚', 'ã ã£ã¦', 'ã£ã¦', 'ãªã‚‰', 'ãŸã‚‰', 'ã‚Œã°', 'ã‘ã‚Œã©',\n",
    "        'ãŒ', 'ã‘ã©', 'ã®ã«', 'ã®ã§', 'ã‹ã‚‰', 'ãŸã‚', 'ã‚ˆã†ã«', 'ãŸã‚ã«', 'ã¨ã—ã¦', 'ã«ã¤ã„ã¦',\n",
    "        'ã«ã‚ˆã£ã¦', 'ã«é–¢ã—ã¦', 'ã«å¯¾ã—ã¦', 'ã«ã¤ã„ã¦', 'ã¨ã—ã¦', 'ã«ãŠã„ã¦', 'ã«ãŠã„ã¦'\n",
    "    }\n",
    "    if japanese_name in japanese_particles:\n",
    "        return None\n",
    "    \n",
    "    # Filter 4: Loáº¡i bá» cÃ¡c tá»« Ä‘Æ¡n láº» khÃ´ng cÃ³ Ã½ nghÄ©a entity\n",
    "    single_meaningless_chars = {\n",
    "        'ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ã', 'ã', 'ã‘', 'ã“', 'ã•', 'ã—', 'ã™', 'ã›', 'ã',\n",
    "        'ãŸ', 'ã¡', 'ã¤', 'ã¦', 'ã¨', 'ãª', 'ã«', 'ã¬', 'ã­', 'ã®', 'ã¯', 'ã²', 'ãµ', 'ã¸', 'ã»',\n",
    "        'ã¾', 'ã¿', 'ã‚€', 'ã‚', 'ã‚‚', 'ã‚„', 'ã‚†', 'ã‚ˆ', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚', 'ã‚’', 'ã‚“'\n",
    "    }\n",
    "    if japanese_name in single_meaningless_chars:\n",
    "        return None\n",
    "    \n",
    "    # Filter 5: Chá»‰ xá»­ lÃ½ náº¿u cÃ³ kÃ½ tá»± chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng sáº¯t\n",
    "    railway_indicators = ['é§…', 'ç·š', 'ãƒ¡ãƒˆãƒ­', 'Metro', 'é‰„é“', 'é›»è»Š', 'æ–°å¹¹ç·š', 'JR', 'æ–¹é¢', 'åœ°ä¸‹é‰„']\n",
    "    has_railway_indicator = any(indicator in japanese_name for indicator in railway_indicators)\n",
    "    \n",
    "    # Filter 6: Kiá»ƒm tra Ä‘á»™ dÃ i há»£p lÃ½ cho tÃªn ga/tuyáº¿n\n",
    "    is_reasonable_length = len(japanese_name) >= 2 and len(japanese_name) <= 25\n",
    "    \n",
    "    # Chá»‰ tiáº¿p tá»¥c náº¿u cÃ³ chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng sáº¯t VÃ€ Ä‘á»™ dÃ i há»£p lÃ½\n",
    "    if not (has_railway_indicator and is_reasonable_length):\n",
    "        return None\n",
    "    \n",
    "    search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"language\": \"ja\",\n",
    "        \"format\": \"json\",\n",
    "        \"search\": japanese_name,\n",
    "        \"limit\": 5 # Get a few results to be safer\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Added explicit headers to mimic a browser request, might help with some firewalls/blocks\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(search_url, params=search_params, timeout=8, headers=headers) # Increased timeout slightly\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        results = response.json().get(\"search\", [])\n",
    "\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        # Pick the first result vÃ  kiá»ƒm tra description\n",
    "        entity_id = results[0][\"id\"]\n",
    "        description = results[0].get(\"description\", \"\").lower()\n",
    "        \n",
    "        # Filter 4: Kiá»ƒm tra description cÃ³ liÃªn quan Ä‘áº¿n Ä‘Æ°á»ng sáº¯t khÃ´ng\n",
    "        railway_keywords = ['station', 'railway', 'train', 'metro', 'line', 'subway', 'transit']\n",
    "        is_railway_related = any(keyword in description for keyword in railway_keywords) if description else True\n",
    "        \n",
    "        # Náº¿u cÃ³ description nhÆ°ng khÃ´ng liÃªn quan Ä‘áº¿n Ä‘Æ°á»ng sáº¯t, bá» qua\n",
    "        if description and not is_railway_related and not has_railway_indicator:\n",
    "            return None\n",
    "\n",
    "        data_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
    "\n",
    "        response = requests.get(data_url, timeout=8, headers=headers) # Added timeout and headers\n",
    "        response.raise_for_status()\n",
    "\n",
    "        entity_data = response.json()\n",
    "\n",
    "        # Try to get the English label\n",
    "        en_label = entity_data.get(\"entities\", {}).get(entity_id, {}).get(\"labels\", {}).get(\"en\", {}).get(\"value\")\n",
    "\n",
    "        if en_label:\n",
    "             return en_label\n",
    "        else:\n",
    "             return None # Found entity but no English label\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Wikidata request timed out for '{japanese_name}'.\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Wikidata connection error for '{japanese_name}'. Network issue?\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "         print(f\"Wikidata HTTP error for '{japanese_name}': {e}\")\n",
    "         return None\n",
    "    except ValueError: # JSON decode error\n",
    "        print(f\"Wikidata response is not valid JSON for '{japanese_name}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Wikidata lookup for '{japanese_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dffb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_entity(jp_entity):\n",
    "    \"\"\"\n",
    "    Translates a Japanese entity name (like station, line, etc.) to English\n",
    "    using a prioritized approach:\n",
    "    1. Custom CSV Dictionary lookup (Kanji -> English) - Highest priority for specific names\n",
    "    2. Wikidata lookup - Good for known proper nouns not in CSV\n",
    "    \"\"\"\n",
    "    jp_entity = jp_entity.strip()\n",
    "    if not jp_entity:\n",
    "        return \"\" \n",
    "    \n",
    "    print(f\"Translating entity: '{jp_entity}'\") # Debug print\n",
    "\n",
    "    # 1) Tra tá»« Ä‘iá»ƒn tÃ¹y chá»‰nh (CSV)\n",
    "    if jp_entity in entity_direct_map_csv:\n",
    "        print(f\" -> Matched in CSV dictionary: {entity_direct_map_csv[jp_entity]}\")\n",
    "        return entity_direct_map_csv[jp_entity]\n",
    "\n",
    "    # 2) Tra tá»« Wikidata\n",
    "    wikidata_result = get_en_name_from_wikidata(jp_entity)\n",
    "    if wikidata_result:\n",
    "        print(f\" -> Matched in Wikidata: {wikidata_result}\")\n",
    "        return wikidata_result\n",
    "\n",
    "    # 3) Fallback to machine translation\n",
    "    return jp_entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75ab4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\"Chia cÃ¢u theo 'ã€‚', giá»¯ dáº¥u cuá»‘i Ä‘á»ƒ ghÃ©p láº¡i máº¡ch láº¡c.\"\"\"\n",
    "    # This function seems designed for splitting input text for NMT batching,\n",
    "    # not directly for entity translation.\n",
    "    parts = [p.strip() for p in text.split(\"ã€‚\") if p.strip()]\n",
    "    return [p + \"ã€‚\" for p in parts]\n",
    "\n",
    "def translate_text(text_list, batch_size=16):\n",
    "    \"\"\"Dá»‹ch list cÃ¢u Nháº­t â†’ Anh báº±ng model.generate() (Batch Processing)\"\"\"\n",
    "    # This function is for batch processing sentences, not used by process_row currently.\n",
    "    if not isinstance(text_list, list):\n",
    "        print(\"Error: translate_text expects a list of strings.\")\n",
    "        return [] # Or raise an error\n",
    "    if model is None or tokenizer is None:\n",
    "         print(\"Error: Model or tokenizer not loaded. Cannot perform batch translation.\")\n",
    "         return [\"Error: Translator not available.\"] * len(text_list)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    # Use the global 'tokenizer' variable\n",
    "    # Note: Splitting by 'ã€‚' might not be ideal for short railway announcements.\n",
    "    # Consider if you need this splitting logic here or just pass full sentences.\n",
    "    # Current code does splitting.\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i : i + batch_size]\n",
    "        seg_pairs = []\n",
    "        concat_map = [] # maps segment index back to original batch_texts index\n",
    "        for idx, txt in enumerate(batch_texts):\n",
    "            # Assuming split_sentences is appropriate here\n",
    "            segs = split_sentences(txt)\n",
    "            seg_pairs.extend(segs)\n",
    "            concat_map.extend([idx] * len(segs))\n",
    "\n",
    "        # Handle empty seg_pairs if batch_texts were empty or didn't split\n",
    "        if not seg_pairs:\n",
    "            results.extend([\"\"] * len(batch_texts))\n",
    "            continue\n",
    "\n",
    "        # Encode all segments\n",
    "        try:\n",
    "            inputs = tokenizer(seg_pairs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=128,\n",
    "                    num_beams=6,\n",
    "                    length_penalty=0.8,\n",
    "                    # Add other generation args if needed, matching process_row direct call\n",
    "                )\n",
    "            decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch NMT translation for batch starting with '{batch_texts[0]}': {e}\")\n",
    "\n",
    "        # Merge segments back into original sentence structure\n",
    "        merged = [\"\"] * len(batch_texts)\n",
    "        for seg, idx in zip(decoded, concat_map):\n",
    "             # Careful merging: add space only if the merged string is not empty\n",
    "             if merged[idx]:\n",
    "                 merged[idx] += (\" \" + seg.strip())\n",
    "             else:\n",
    "                 merged[idx] += seg.strip()\n",
    "\n",
    "        # Final cleanup for merged sentences\n",
    "        merged = [re.sub(r'\\s+([.,!?:;])', r'\\1', s).strip() for s in merged] # Punctuation spacing\n",
    "        merged = [re.sub(r'\\s+', ' ', s).strip() for s in merged] # Normalize spaces\n",
    "        merged = [s.replace(' .', '.') for s in merged] # Fix common space before period\n",
    "\n",
    "\n",
    "        results.extend(merged)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5b08213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_adjacent_duplicate_phrases(text, max_phrase_len=5):\n",
    "    \"\"\"\n",
    "    Loáº¡i bá» cÃ¡c cá»¥m tá»« (tá»« 1 Ä‘áº¿n max_phrase_len tá»«) láº·p liÃªn tiáº¿p nhau trong vÄƒn báº£n.\n",
    "    Há»— trá»£ láº·p cÃ³ hoáº·c khÃ´ng cÃ³ dáº¥u pháº©y, khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng.\n",
    "    \"\"\"\n",
    "    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a á»Ÿ dáº¥u pháº©y\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # Xá»­ lÃ½ cá»¥m tá»« láº·p, giáº£m dáº§n tá»« cá»¥m dÃ i nháº¥t vá» 1 tá»«\n",
    "    for n in range(max_phrase_len, 0, -1):\n",
    "        # Táº¡o regex Ä‘á»™ng cho cá»¥m n tá»« láº·p liÃªn tiáº¿p (cÃ³ thá»ƒ cÃ³ dáº¥u pháº©y)\n",
    "        # VÃ­ dá»¥ cho n=2: (Tokyo Metro)(,? Tokyo Metro)\n",
    "        pattern = re.compile(\n",
    "            r'(\\b(?:[\\w\\-\\â€™ÅÅ«]+(?:\\s+|, ?)){%d}[\\w\\-\\â€™ÅÅ«]+\\b)'    # NhÃ³m 1: n tá»«\n",
    "            r'(,? \\1\\b)'                                          # NhÃ³m 2: láº·p láº¡i\n",
    "            % (n-1),\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "        # Láº·p Ä‘á»ƒ loáº¡i táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p láº·p liÃªn tiáº¿p\n",
    "        while pattern.search(text):\n",
    "            text = pattern.sub(r'\\1', text)\n",
    "    # Sau Ä‘Ã³, xá»­ lÃ½ riÃªng láº·p 1 tá»« vá»›i dáº¥u pháº©y hoáº·c khÃ´ng dáº¥u pháº©y\n",
    "    text = re.sub(r'\\b(\\w+)(,? \\1\\b)', r'\\1', text, flags=re.IGNORECASE)\n",
    "    # Sá»­a cÃ¡c trÆ°á»ng há»£p dáº¥u cÃ¢u dÆ°, khoáº£ng tráº¯ng thá»«a\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bdb8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_simple(text):\n",
    "    \"\"\"\n",
    "    Dá»‹ch vÄƒn báº£n Ä‘Æ¡n giáº£n báº±ng machine translation model\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=6,\n",
    "                length_penalty=0.8,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        result = tokenizer.decode(generated[0], skip_special_tokens=True).strip()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Translation error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a037d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_entities_with_fallback(ph2ent):\n",
    "    \"\"\"\n",
    "    Dá»‹ch entities vá»›i logic:\n",
    "    1. Check trong CSV trÆ°á»›c\n",
    "    2. KhÃ´ng cÃ³ thÃ¬ check báº±ng hÃ m Wiki\n",
    "    3. KhÃ´ng cÃ³ thÃ¬ thay ngÆ°á»£c láº¡i tá»« Ä‘Ã³ thay cho placeholder Ä‘Ã£ thay trong cÃ¢u vÃ  xÃ³a nÃ³ trong placeholder map\n",
    "    \n",
    "    Args:\n",
    "        ph2ent: Dictionary mapping placeholder -> entity (vÃ­ dá»¥: {'[PH0]': 'ã¯', '[PH1]': 'å¤§é˜ª'})\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (updated_ph2ent, entities_to_restore)\n",
    "        - updated_ph2ent: Dictionary Ä‘Ã£ loáº¡i bá» entities khÃ´ng dá»‹ch Ä‘Æ°á»£c\n",
    "        - entities_to_restore: Dictionary mapping placeholder -> entity Ä‘á»ƒ thay ngÆ°á»£c láº¡i\n",
    "    \"\"\"\n",
    "    updated_ph2ent = {}\n",
    "    entities_to_restore = {}\n",
    "    \n",
    "    print(\"ğŸ”„ Báº¯t Ä‘áº§u dá»‹ch entities...\")\n",
    "    \n",
    "    for placeholder, entity in ph2ent.items():\n",
    "        print(f\"\\nğŸ“ Xá»­ lÃ½ {placeholder} -> '{entity}'\")\n",
    "        \n",
    "        # BÆ°á»›c 1: Check trong CSV\n",
    "        if entity in entity_direct_map_csv:\n",
    "            translated = entity_direct_map_csv[entity]\n",
    "            updated_ph2ent[placeholder] = translated\n",
    "            print(f\"  âœ… TÃ¬m tháº¥y trong CSV: '{entity}' -> '{translated}'\")\n",
    "            continue\n",
    "        \n",
    "        # BÆ°á»›c 2: Check báº±ng hÃ m Wiki\n",
    "        wikidata_result = get_en_name_from_wikidata(entity)\n",
    "        if wikidata_result:\n",
    "            updated_ph2ent[placeholder] = wikidata_result\n",
    "            print(f\"  âœ… TÃ¬m tháº¥y trong Wiki: '{entity}' -> '{wikidata_result}'\")\n",
    "            continue\n",
    "        \n",
    "        # BÆ°á»›c 3: KhÃ´ng tÃ¬m Ä‘Æ°á»£c -> thay ngÆ°á»£c láº¡i\n",
    "        entities_to_restore[placeholder] = entity\n",
    "        print(f\"  âŒ KhÃ´ng tÃ¬m Ä‘Æ°á»£c báº£n dá»‹ch cho '{entity}' -> sáº½ thay ngÆ°á»£c láº¡i\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Káº¿t quáº£:\")\n",
    "    print(f\"  - Entities dá»‹ch Ä‘Æ°á»£c: {len(updated_ph2ent)}\")\n",
    "    print(f\"  - Entities thay ngÆ°á»£c láº¡i: {len(entities_to_restore)}\")\n",
    "    \n",
    "    return updated_ph2ent, entities_to_restore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80157098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_entities_and_translate(text_with_placeholders, entities_to_restore, translated_entities):\n",
    "    \"\"\"\n",
    "    Thay ngÆ°á»£c láº¡i cÃ¡c entities khÃ´ng dá»‹ch Ä‘Æ°á»£c vÃ  dá»‹ch cÃ¢u\n",
    "    \n",
    "    Args:\n",
    "        text_with_placeholders: CÃ¢u Ä‘Ã£ thay placeholder (vÃ­ dá»¥: \"ã“ã®é›»è»Š[PH0][PH1]è¡Œãå¿«é€Ÿé›»è»Šã§ã™ã€‚\")\n",
    "        entities_to_restore: Dictionary mapping placeholder -> entity Ä‘á»ƒ thay ngÆ°á»£c láº¡i\n",
    "        translated_entities: Dictionary mapping placeholder -> entity Ä‘Ã£ dá»‹ch\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (final_text, final_ph2ent)\n",
    "        - final_text: CÃ¢u Ä‘Ã£ thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c\n",
    "        - final_ph2ent: Dictionary chá»‰ chá»©a entities Ä‘Ã£ dá»‹ch Ä‘Æ°á»£c\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c...\")\n",
    "    \n",
    "    # BÆ°á»›c 1: Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c\n",
    "    final_text = text_with_placeholders\n",
    "    for placeholder, original_entity in entities_to_restore.items():\n",
    "        final_text = final_text.replace(placeholder, original_entity)\n",
    "        print(f\"  ğŸ”„ Thay ngÆ°á»£c {placeholder} -> '{original_entity}'\")\n",
    "    \n",
    "    print(f\"ğŸ“ Text sau khi thay ngÆ°á»£c: {final_text}\")\n",
    "    \n",
    "    # BÆ°á»›c 2: Dá»‹ch cÃ¢u\n",
    "    print(\"ğŸ”„ Dá»‹ch cÃ¢u...\")\n",
    "    translated_text = translate_text_simple(final_text)\n",
    "    print(f\"ğŸ“ CÃ¢u Ä‘Ã£ dá»‹ch: {translated_text}\")\n",
    "    \n",
    "    # BÆ°á»›c 3: Táº¡o final mapping chá»‰ chá»©a entities Ä‘Ã£ dá»‹ch Ä‘Æ°á»£c\n",
    "    final_ph2ent = translated_entities.copy()\n",
    "    \n",
    "    return final_text, translated_text, final_ph2ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a69fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_translation_with_entities(translated_text, final_ph2ent):\n",
    "    \"\"\"\n",
    "    GhÃ©p láº¡i káº¿t quáº£ dá»‹ch vá»›i entities Ä‘Ã£ dá»‹ch dá»±a trÃªn placeholder mapping\n",
    "    \n",
    "    Args:\n",
    "        translated_text: CÃ¢u Ä‘Ã£ dá»‹ch (vÃ­ dá»¥: \"This train is a rapid train bound for [PH1]. Next is [PH2], [PH2].\")\n",
    "        final_ph2ent: Dictionary mapping placeholder -> entity Ä‘Ã£ dá»‹ch (vÃ­ dá»¥: {'[PH1]': 'Osaka', '[PH2]': 'Kyoto'})\n",
    "    \n",
    "    Returns:\n",
    "        str: CÃ¢u cuá»‘i cÃ¹ng Ä‘Ã£ ghÃ©p entities\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch...\")\n",
    "    \n",
    "    final_result = translated_text\n",
    "    \n",
    "    # Thay tháº¿ cÃ¡c placeholder báº±ng entities Ä‘Ã£ dá»‹ch\n",
    "    for placeholder, translated_entity in final_ph2ent.items():\n",
    "        if placeholder in final_result:\n",
    "            final_result = final_result.replace(placeholder, translated_entity)\n",
    "            print(f\"  âœ… Thay {placeholder} -> '{translated_entity}'\")\n",
    "    \n",
    "    # Loáº¡i bá» cÃ¡c placeholder cÃ²n láº¡i (náº¿u cÃ³)\n",
    "    import re\n",
    "    remaining_placeholders = re.findall(r'\\[PH\\d+\\]', final_result)\n",
    "    if remaining_placeholders:\n",
    "        print(f\"  âš ï¸ CÃ²n láº¡i placeholders: {remaining_placeholders}\")\n",
    "    \n",
    "    print(f\"ğŸ“ Káº¿t quáº£ cuá»‘i cÃ¹ng: {final_result}\")\n",
    "    \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ad893a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_entity_handling(text):\n",
    "    \"\"\"\n",
    "    HÃ m tá»•ng há»£p thá»±c hiá»‡n toÃ n bá»™ flow dá»‹ch vá»›i xá»­ lÃ½ entities:\n",
    "    \n",
    "    1. Thay tháº¿ entities báº±ng placeholders\n",
    "    2. Dá»‹ch entities (check CSV -> Wiki -> thay ngÆ°á»£c láº¡i)\n",
    "    3. Dá»‹ch cÃ¢u Ä‘Ã£ thay placeholder\n",
    "    4. GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch\n",
    "    \n",
    "    Args:\n",
    "        text: CÃ¢u tiáº¿ng Nháº­t cáº§n dá»‹ch\n",
    "    \n",
    "    Returns:\n",
    "        str: CÃ¢u tiáº¿ng Anh Ä‘Ã£ dá»‹ch hoÃ n chá»‰nh\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ Báº®T Äáº¦U QUÃ TRÃŒNH Dá»ŠCH Vá»šI Xá»¬ LÃ ENTITIES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“ Input: {text}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # BÆ°á»›c 1: Thay tháº¿ entities báº±ng placeholders\n",
    "        print(\"ğŸ”¸ BÆ¯á»šC 1: Thay tháº¿ entities báº±ng placeholders\")\n",
    "        text_with_placeholders, ph2ent = replace_entity_and_map(text)\n",
    "        print(f\"ğŸ“ Text sau khi thay placeholder: {text_with_placeholders}\")\n",
    "        print(f\"ğŸ—ºï¸ Placeholder mapping: {ph2ent}\")\n",
    "        print()\n",
    "        \n",
    "        if not ph2ent:\n",
    "            print(\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y entities, dá»‹ch trá»±c tiáº¿p...\")\n",
    "            return translate_text_simple(text)\n",
    "        \n",
    "        # BÆ°á»›c 2: Dá»‹ch entities\n",
    "        print(\"ğŸ”¸ BÆ¯á»šC 2: Dá»‹ch entities\")\n",
    "        translated_entities, entities_to_restore = translate_entities_with_fallback(ph2ent)\n",
    "        print()\n",
    "        \n",
    "        # BÆ°á»›c 3: Thay ngÆ°á»£c láº¡i vÃ  dá»‹ch cÃ¢u\n",
    "        print(\"ğŸ”¸ BÆ¯á»šC 3: Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c vÃ  dá»‹ch cÃ¢u\")\n",
    "        final_text, translated_text, final_ph2ent = restore_entities_and_translate(\n",
    "            text_with_placeholders, entities_to_restore, translated_entities\n",
    "        )\n",
    "        print()\n",
    "        \n",
    "        # BÆ°á»›c 4: GhÃ©p láº¡i káº¿t quáº£\n",
    "        print(\"ğŸ”¸ BÆ¯á»šC 4: GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch\")\n",
    "        final_result = merge_translation_with_entities(translated_text, final_ph2ent)\n",
    "        print()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… HOÃ€N THÃ€NH QUÃ TRÃŒNH Dá»ŠCH\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ğŸ“ Káº¿t quáº£ cuá»‘i cÃ¹ng: {final_result}\")\n",
    "        \n",
    "        return final_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i trong quÃ¡ trÃ¬nh dá»‹ch: {e}\")\n",
    "        print(\"ğŸ”„ Fallback: Dá»‹ch trá»±c tiáº¿p...\")\n",
    "        return translate_text_simple(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b15ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª TEST Vá»šI VÃ Dá»¤ Tá»ª YÃŠU Cáº¦U\n",
      "==================================================\n",
      "============================================================\n",
      "ğŸš€ Báº®T Äáº¦U QUÃ TRÃŒNH Dá»ŠCH Vá»šI Xá»¬ LÃ ENTITIES\n",
      "============================================================\n",
      "ğŸ“ Input:  ã¾ã‚‚ãªãã€å“å·ã€å“å·ã€‚ãŠå‡ºå£ã¯å³å´ã§ã™ã€‚äº¬æµœæ±åŒ—ç·šã€å±±æ‰‹ç·šã€äº¬æ€¥ç·šã¯ãŠä¹—ã‚Šæ›ãˆã§ã™ã€‚æœ¬æ—¥ã‚‚JRæ±æ—¥æœ¬ã‚’ã”åˆ©ç”¨ãã ã•ã„ã¾ã—ã¦ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚\n",
      "\n",
      "ğŸ”¸ BÆ¯á»šC 1: Thay tháº¿ entities báº±ng placeholders\n",
      "ğŸ“ Text sau khi thay placeholder:  ã¾[PH0]ãªã[PH1][PH2][PH1][PH2][PH3]ãŠå‡ºå£ã¯å³å´ã§ã™[PH3][PH4][PH1][PH5][PH1][PH6]ã¯ãŠä¹—ã‚Šæ›ãˆã§ã™[PH3]æœ¬æ—¥[PH0][PH7]ã‚’ã”åˆ©ç”¨ãã ã•ã„ã¾ã—ã¦[PH1]ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ[PH3]\n",
      "ğŸ—ºï¸ Placeholder mapping: {'[PH0]': 'ã‚‚', '[PH1]': 'ã€', '[PH2]': 'å“å·', '[PH3]': 'ã€‚', '[PH4]': 'äº¬æµœæ±åŒ—ç·š', '[PH5]': 'å±±æ‰‹ç·š', '[PH6]': 'äº¬æ€¥ç·š', '[PH7]': 'JRæ±æ—¥æœ¬'}\n",
      "\n",
      "ğŸ”¸ BÆ¯á»šC 2: Dá»‹ch entities\n",
      "ğŸ”„ Báº¯t Ä‘áº§u dá»‹ch entities...\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH0] -> 'ã‚‚'\n",
      "  âŒ KhÃ´ng tÃ¬m Ä‘Æ°á»£c báº£n dá»‹ch cho 'ã‚‚' -> sáº½ thay ngÆ°á»£c láº¡i\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH1] -> 'ã€'\n",
      "  âŒ KhÃ´ng tÃ¬m Ä‘Æ°á»£c báº£n dá»‹ch cho 'ã€' -> sáº½ thay ngÆ°á»£c láº¡i\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH2] -> 'å“å·'\n",
      "  âœ… TÃ¬m tháº¥y trong CSV: 'å“å·' -> 'Shinagawa'\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH3] -> 'ã€‚'\n",
      "  âŒ KhÃ´ng tÃ¬m Ä‘Æ°á»£c báº£n dá»‹ch cho 'ã€‚' -> sáº½ thay ngÆ°á»£c láº¡i\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH4] -> 'äº¬æµœæ±åŒ—ç·š'\n",
      "  âœ… TÃ¬m tháº¥y trong CSV: 'äº¬æµœæ±åŒ—ç·š' -> 'Keihinâ€“TÅhoku Line'\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH5] -> 'å±±æ‰‹ç·š'\n",
      "  âœ… TÃ¬m tháº¥y trong CSV: 'å±±æ‰‹ç·š' -> 'Yamate Line'\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH6] -> 'äº¬æ€¥ç·š'\n",
      "  âœ… TÃ¬m tháº¥y trong Wiki: 'äº¬æ€¥ç·š' -> 'KeikyÅ« Main Line'\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH7] -> 'JRæ±æ—¥æœ¬'\n",
      "  âœ… TÃ¬m tháº¥y trong Wiki: 'äº¬æ€¥ç·š' -> 'KeikyÅ« Main Line'\n",
      "\n",
      "ğŸ“ Xá»­ lÃ½ [PH7] -> 'JRæ±æ—¥æœ¬'\n",
      "  âœ… TÃ¬m tháº¥y trong Wiki: 'JRæ±æ—¥æœ¬' -> 'East Japan Railway Company'\n",
      "\n",
      "ğŸ“Š Káº¿t quáº£:\n",
      "  - Entities dá»‹ch Ä‘Æ°á»£c: 5\n",
      "  - Entities thay ngÆ°á»£c láº¡i: 3\n",
      "\n",
      "ğŸ”¸ BÆ¯á»šC 3: Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c vÃ  dá»‹ch cÃ¢u\n",
      "ğŸ”„ Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c...\n",
      "  ğŸ”„ Thay ngÆ°á»£c [PH0] -> 'ã‚‚'\n",
      "  ğŸ”„ Thay ngÆ°á»£c [PH1] -> 'ã€'\n",
      "  ğŸ”„ Thay ngÆ°á»£c [PH3] -> 'ã€‚'\n",
      "ğŸ“ Text sau khi thay ngÆ°á»£c:  ã¾ã‚‚ãªãã€[PH2]ã€[PH2]ã€‚ãŠå‡ºå£ã¯å³å´ã§ã™ã€‚[PH4]ã€[PH5]ã€[PH6]ã¯ãŠä¹—ã‚Šæ›ãˆã§ã™ã€‚æœ¬æ—¥ã‚‚[PH7]ã‚’ã”åˆ©ç”¨ãã ã•ã„ã¾ã—ã¦ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚\n",
      "ğŸ”„ Dá»‹ch cÃ¢u...\n",
      "  âœ… TÃ¬m tháº¥y trong Wiki: 'JRæ±æ—¥æœ¬' -> 'East Japan Railway Company'\n",
      "\n",
      "ğŸ“Š Káº¿t quáº£:\n",
      "  - Entities dá»‹ch Ä‘Æ°á»£c: 5\n",
      "  - Entities thay ngÆ°á»£c láº¡i: 3\n",
      "\n",
      "ğŸ”¸ BÆ¯á»šC 3: Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c vÃ  dá»‹ch cÃ¢u\n",
      "ğŸ”„ Thay ngÆ°á»£c láº¡i entities khÃ´ng dá»‹ch Ä‘Æ°á»£c...\n",
      "  ğŸ”„ Thay ngÆ°á»£c [PH0] -> 'ã‚‚'\n",
      "  ğŸ”„ Thay ngÆ°á»£c [PH1] -> 'ã€'\n",
      "  ğŸ”„ Thay ngÆ°á»£c [PH3] -> 'ã€‚'\n",
      "ğŸ“ Text sau khi thay ngÆ°á»£c:  ã¾ã‚‚ãªãã€[PH2]ã€[PH2]ã€‚ãŠå‡ºå£ã¯å³å´ã§ã™ã€‚[PH4]ã€[PH5]ã€[PH6]ã¯ãŠä¹—ã‚Šæ›ãˆã§ã™ã€‚æœ¬æ—¥ã‚‚[PH7]ã‚’ã”åˆ©ç”¨ãã ã•ã„ã¾ã—ã¦ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚\n",
      "ğŸ”„ Dá»‹ch cÃ¢u...\n",
      "ğŸ“ CÃ¢u Ä‘Ã£ dá»‹ch: We will soon be arriving at [PH2], [PH2]. The doors will open on the right. Please transfer here for the [PH4], [PH5], and [PH6]. Thank you for traveling with [PH7] today.\n",
      "\n",
      "ğŸ”¸ BÆ¯á»šC 4: GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch\n",
      "ğŸ”„ GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch...\n",
      "  âœ… Thay [PH2] -> 'Shinagawa'\n",
      "  âœ… Thay [PH4] -> 'Keihinâ€“TÅhoku Line'\n",
      "  âœ… Thay [PH5] -> 'Yamate Line'\n",
      "  âœ… Thay [PH6] -> 'KeikyÅ« Main Line'\n",
      "  âœ… Thay [PH7] -> 'East Japan Railway Company'\n",
      "ğŸ“ Káº¿t quáº£ cuá»‘i cÃ¹ng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihinâ€“TÅhoku Line, Yamate Line, and KeikyÅ« Main Line. Thank you for traveling with East Japan Railway Company today.\n",
      "\n",
      "============================================================\n",
      "âœ… HOÃ€N THÃ€NH QUÃ TRÃŒNH Dá»ŠCH\n",
      "============================================================\n",
      "ğŸ“ Káº¿t quáº£ cuá»‘i cÃ¹ng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihinâ€“TÅhoku Line, Yamate Line, and KeikyÅ« Main Line. Thank you for traveling with East Japan Railway Company today.\n",
      "ğŸ“ CÃ¢u Ä‘Ã£ dá»‹ch: We will soon be arriving at [PH2], [PH2]. The doors will open on the right. Please transfer here for the [PH4], [PH5], and [PH6]. Thank you for traveling with [PH7] today.\n",
      "\n",
      "ğŸ”¸ BÆ¯á»šC 4: GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch\n",
      "ğŸ”„ GhÃ©p láº¡i káº¿t quáº£ vá»›i entities Ä‘Ã£ dá»‹ch...\n",
      "  âœ… Thay [PH2] -> 'Shinagawa'\n",
      "  âœ… Thay [PH4] -> 'Keihinâ€“TÅhoku Line'\n",
      "  âœ… Thay [PH5] -> 'Yamate Line'\n",
      "  âœ… Thay [PH6] -> 'KeikyÅ« Main Line'\n",
      "  âœ… Thay [PH7] -> 'East Japan Railway Company'\n",
      "ğŸ“ Káº¿t quáº£ cuá»‘i cÃ¹ng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihinâ€“TÅhoku Line, Yamate Line, and KeikyÅ« Main Line. Thank you for traveling with East Japan Railway Company today.\n",
      "\n",
      "============================================================\n",
      "âœ… HOÃ€N THÃ€NH QUÃ TRÃŒNH Dá»ŠCH\n",
      "============================================================\n",
      "ğŸ“ Káº¿t quáº£ cuá»‘i cÃ¹ng: We will soon be arriving at Shinagawa, Shinagawa. The doors will open on the right. Please transfer here for the Keihinâ€“TÅhoku Line, Yamate Line, and KeikyÅ« Main Line. Thank you for traveling with East Japan Railway Company today.\n"
     ]
    }
   ],
   "source": [
    "# Test vá»›i vÃ­ dá»¥ tá»« yÃªu cáº§u\n",
    "test_text = \" ã¾ã‚‚ãªãã€å“å·ã€å“å·ã€‚ãŠå‡ºå£ã¯å³å´ã§ã™ã€‚äº¬æµœæ±åŒ—ç·šã€å±±æ‰‹ç·šã€äº¬æ€¥ç·šã¯ãŠä¹—ã‚Šæ›ãˆã§ã™ã€‚æœ¬æ—¥ã‚‚JRæ±æ—¥æœ¬ã‚’ã”åˆ©ç”¨ãã ã•ã„ã¾ã—ã¦ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚\"\n",
    "\n",
    "print(\"ğŸ§ª TEST Vá»šI VÃ Dá»¤ Tá»ª YÃŠU Cáº¦U\")\n",
    "print(\"=\" * 50)\n",
    "result = translate_with_entity_handling(test_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
